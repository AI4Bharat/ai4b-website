<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"
        integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

    <link rel="stylesheet" href="../styles/ai4b-base.css">
    <link rel="stylesheet" href="../styles/ai4b-timeline.css">
    <title>Language Models</title>
</head>

<body>
    <div id="head_content"></div>
    <div class="main">
        <div class="asr-section">
            <div class="left-panel">
                <h1>Language Models</h1>
            </div>
            <div class="right-panel">
                <p>
                    At AI4Bharat, our dedication to building language models and datasets for all 22 constitutionally
                    recognized Indian languages is central to our mission. We employ a multifaceted approach, leveraging
                    large-scale data crawling, synthetic data creation, and human annotation/crowd collections to create
                    comprehensive datasets. Our efforts have resulted in an extensive pretraining corpus of 251 million
                    tokens across 22 languages, complemented by 74.7 million prompt-response pairs in 20 Indian
                    languages. Tools like Setu play a crucial role in large-scale crawling and data cleaning, enabling
                    us to build state-of-the-art models such as Airavata, IndicBART, and IndicBERT. We also emphasize
                    rigorous evaluation of our models, as demonstrated by our works like FBI, IndicXTREME, IndicNLG, and
                    IndicGLUE, which set new benchmarks in language model performance. Looking ahead, we are committed
                    to expanding our pretraining corpora to support the development of even more robust generative
                    models, while ensuring diversity in their generation capabilities, thereby advancing the frontier of
                    language technology for Indiaâ€™s diverse linguistic landscape. </p>
                <button class="cta-button">Try out our latest model</button>
                <div class="highlight">To know more about our contributions over the years see the timeline below!</div>
            </div>
        </div>
        <!-- TODO: links not showing up for papers and github -->
        <!-- TODO: change all dataset links/imgs from github to HF -->
        <!-- TODO: Create collections for benchmarks, e.g. IndicNLG -->
        <!-- TODO: date is overflowing, fix that! -->
        <div class="container">
            <ul>
                <li>
                    <h3 class="heading">FBI</h3>
                    <p>FBI is an adversarial test set to evaluate LLMs' proficiency in assessing factual accuracy,
                        instruction following, coherence in long-form writing, and reasoning. It contains a total of
                        2.4k examples across the 4 task categories.</p>
                    <a href="https://github.com/AI4Bharat/FBI"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2406.13439"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date dataset">June 2024</span>
                    <span class="circle"></span>
                </li>

                <li>
                    <h3 class="heading">Sangraha</h3>
                    <p>Sangraha is pre-training data corpus containing 251B tokens summed up over 22 languages,
                        extracted from human curated URLs, existing multilingual corpora, and large-scale translations.
                    </p>
                    <a href="https://huggingface.co/datasets/ai4bharat/sangraha"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2403.06350"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">March 2024</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicAlign - Instruct</h3>
                    <p>IndicAlign Instruct is a diverse collection of 74.7M prompt-response pairs across 20 languages,
                        gathered through four methods: aggregating existing IFT datasets, translating English datasets
                        into 14 Indian languages, generating conversations from India-centric Wikipedia articles using
                        open-source LLMs, and crowdsourcing prompts via the Anudesh platform.</p>
                    <a href="https://huggingface.co/datasets/ai4bharat/indic-align"><i
                            class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2403.06350"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">March 2024</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">Setu</h3>
                    <p>Airavata is an instruction-tuned LLM for Hindi, created by fine-tuning OpenHathi with diverse
                        Hindi instruction-tuning datasets for enhanced instruction following performance.</p>
                    <a href="https://github.com/AI4Bharat/setu"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2403.06350"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">March 2024</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">RomanSetu</h3>
                    <p>RomanSetu proposes an approach that utilizes the romanized form of text as an interface for LLMs,
                        hypothesizing that its frequent informal use and shared tokens with English enhance crosslingual
                        alignment. The results indicate that romanized text not only reduces token fertility by 2x-4x
                        but also matches or outperforms native script representation across various NLU, NLG, and MT
                        tasks.</p>
                    <a href="https://github.com/AI4Bharat/romansetu"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2401.14280"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">January 2024</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">Airavata</h3>
                    <p>Airavata is an instruction-tuned LLM for Hindi, created by fine-tuning OpenHathi with diverse
                        Hindi instruction-tuning datasets for enhanced instruction following performance.</p>
                    <a href="https://github.com/AI4Bharat/IndicInstruct"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2401.15006"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">January 2024</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">Bhasha-Abhijnaanam</h3>
                    <p>Bhasha-Abhijnaanam is a language identification test set for native-script as well as romanized
                        text which spans 22 Indic languages.</p>
                    <a href="https://github.com/AI4Bharat/IndicLID"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-short.71"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">May 2023</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicLID</h3>
                    <p>IndicLID is a publicly available language identification datasets for all 22 Indian languages in
                        both native-script and romanized text.</p>
                    <a href="https://github.com/AI4Bharat/IndicLID"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-short.71"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">May 2023</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicCorp v2</h3>
                    <p>IndicCorp v2 is the largest monolingual corpora for Indian languages (at the time of release),
                        with 20.9B tokens covering 24 languages from 4 language families.</p>
                    <a href="https://github.com/AI4Bharat/IndicBERT"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-long.693"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicXTREME</h3>
                    <p>IndicXTREME is a human-supervised benchmark of 9 diverse NLU tasks across 20 languages, featuring
                        105 evaluation sets in total, including 52 new contributions.</p>
                    <a href="https://github.com/AI4Bharat/IndicBERT"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-long.693"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicBERT v2</h3>
                    <p>IndicBERT v2 is a multilingual BERT model pretrained on IndicCorp v2, an Indic monolingual corpus
                        of 20.9 billion tokens, covering 24 consitutionally recognised Indian languages. It has been
                        evaluated on a diverse set of tasks from IndicXTREME to assess its performance.</p>
                    <a href="https://github.com/AI4Bharat/IndicBERT"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-long.693"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">Naamapadam</h3>
                    <p>Naamapadam is the largest publicly available NER dataset for the 11 major Indian languages from
                        two language families. The dataset contains more than 400k sentences annotated with a total of
                        at least 100k entities from three standard entity categories (Person, Location, and,
                        Organization) for 9 out of the 11 languages.</p>
                    <a href="https://huggingface.co/ai4bharat/IndicNER"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-long.582"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicNER</h3>
                    <p>IndicNER is a multilingual IndicBERT model fine-tuned on the Naamapadam training set, achieving
                        an F1 score over 80 for 7 of 9 test languages.</p>
                    <a href="https://huggingface.co/ai4bharat/IndicNER"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2023.acl-long.582"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">Aksharantar</h3>
                    <p>Aksharantar is the largest publicly available transliteration dataset for 21 Indic languages. The
                        corpus has 26M Indic language-English transliteration pairs.</p>
                    <a href="https://github.com/AI4Bharat/IndicXlit"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2205.03018"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">May 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicXlit</h3>
                    <p>IndicXlit is a transformer-based multilingual transliteration model that supports 21 Indic
                        languages for Roman to native script and native to Roman script conversions. It is trained on
                        Aksharantar dataset which is the largest publicly available parallel corpus containing 26
                        million word pairs spanning 20 Indic languages.</p>
                    <a href="https://github.com/AI4Bharat/IndicXlit"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2205.03018"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">May 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicNLG</h3>
                    <p>IndicNLG Benchmark is a dataset collection designed for benchmarking Natural Language Generation
                        (NLG) across 11 Indic languages. It covers five diverse tasks: biography generation using
                        Wikipedia infoboxes, news headline generation, sentence summarization, paraphrase generation
                        and, question generation.</p>
                    <a href="https://github.com/AI4Bharat/indic-bart"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2022.emnlp-main.360"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">March 2022</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicBART</h3>
                    <p>IndicBART is a multilingual, sequence-to-sequence pre-trained model designed for 11 Indic
                        languages and English. By leveraging the orthographic similarity between Indic scripts,
                        IndicBART enhances transfer learning across similar Indic languages, making it a powerful tool
                        for multilingual natural language processing.</p>
                    <a href="https://github.com/AI4Bharat/indic-bart"><i class="fa-brands fa-github"></i></a>
                    <a href="https://arxiv.org/abs/2109.02903"><i class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">September 2021</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicGLUE</h3>
                    <p>IndicGLUE is the Indic General Language Understanding Evaluation Benchmark, featuring 6 NLP tasks
                        across 11 Indian languages.</p>
                    <a href="IndicCorp"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2020.findings-emnlp.445"><i
                            class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2020</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicBERT</h3>
                    <p>IndicBERT is a multilingual ALBERT model pretrained on an Indic monolingual corpus of around 9
                        billion tokens, covering 12 major Indian languages. It has been evaluated on a diverse set of
                        tasks from IndicGLUE to assess its performance.</p>
                    <a href="https://github.com/AI4Bharat/Indic-BERT-v1"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2020.findings-emnlp.445"><i
                            class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2020</span>
                    <span class="circle"></span>
                </li>
                <li>
                    <h3 class="heading">IndicCorp</h3>
                    <p>IndicCorp is a monolingual corpus with 8.8B tokens across 11 languages and Indian English,
                        primarily sourced from news crawls.</p>
                    <a href="IndicCorp"><i class="fa-brands fa-github"></i></a>
                    <a href="https://aclanthology.org/2020.findings-emnlp.445"><i
                            class="fa-regular fa-file-pdf"></i></a>
                    <span class="date model">December 2020</span>
                    <span class="circle"></span>
                </li>
            </ul>
        </div>
    </div>
    <div id="foot_content"></div>
</body>

<script type="text/javascript">
    $('#head_content').load('header.html')
    $('#foot_content').load('footer.html')
</script>

</html>